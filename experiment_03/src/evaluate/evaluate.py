"""
æ¨¡å‹è¯„ä¼°ï¼šè®¡ç®— MSEã€RÂ² ç­‰æŒ‡æ ‡ï¼Œå¹¶å°†æŒ‡æ ‡è®°å½•åˆ° MLflow
"""
import argparse
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import mlflow
import joblib
import json
import os

# é…ç½® MLflow
TRACKING_URI = "http://localhost:5555"
EXPERIMENT_NAME = "housing-price-experiment"
RUN_NAME = "housing_price_rf_test"

mlflow.set_tracking_uri(TRACKING_URI)
mlflow.set_experiment(EXPERIMENT_NAME)


def find_run_by_params(n_estimators, max_depth):
    """æ ¹æ®å‚æ•°æŸ¥æ‰¾å¯¹åº”çš„ MLflow Run"""
    client = mlflow.tracking.MlflowClient()
    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)
    if not experiment:
        raise ValueError(f"å®éªŒ '{EXPERIMENT_NAME}' ä¸å­˜åœ¨")

    # æ„å»ºæœç´¢è¡¨è¾¾å¼
    search_filter = (
        f"params.n_estimators = '{n_estimators}' "
        f"and params.max_depth = '{max_depth}' "
        f"and tags.mlflow.runName = '{RUN_NAME}' "
    )

    runs = client.search_runs(
        experiment_ids=[experiment.experiment_id],
        filter_string=search_filter,
        max_results=1,
        order_by=["attributes.start_time DESC"]
    )

    if len(runs) == 0:
        raise ValueError(f"æœªæ‰¾åˆ° n_estimators={n_estimators}, max_depth={max_depth} çš„è®­ç»ƒè®°å½•")

    return runs[0]


def evaluate_model(n_estimators=100, max_depth=5):
    print(f"ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹: n_estimators={n_estimators}, max_depth={max_depth}")

    # âœ… åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆæŒ‰å‚æ•°å‘½åï¼‰
    model_filename = f"rf_model_n{n_estimators}_d{max_depth}.pkl"
    model_path = os.path.join("models", model_filename)
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨ï¼Œè¯·è®­ç»ƒåå†è¿›è¡Œè¯„ä¼°ï¼")

    model = joblib.load(model_path)

    # åŠ è½½æµ‹è¯•æ•°æ®
    x_test = pd.read_csv('data/processed/x_test.csv')
    y_test = pd.read_csv('data/processed/y_test.csv').values.ravel()

    # é¢„æµ‹
    y_pred = model.predict(x_test)

    # è®¡ç®—æŒ‡æ ‡
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    metrics = {"mse": mse, "mae": mae, "rmse": rmse, "r2": r2}

    # âœ… åœ¨ MLflow ä¸­è®°å½•æŒ‡æ ‡ï¼ˆå…³è”åˆ°è®­ç»ƒ Runï¼‰
    try:
        run = find_run_by_params(n_estimators, max_depth)
        with mlflow.start_run(run_id=run.info.run_id):
            mlflow.log_metrics(metrics)
            mlflow.set_tag("evaluation", "test_set")
            mlflow.log_param("eval_dataset", "test_set_v1")
        print(f"âœ… æŒ‡æ ‡å·²è®°å½•åˆ° MLflow Run ID: {run.info.run_id}")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è®°å½•åˆ° MLflow: {e}")

    # ä¿å­˜æœ¬åœ°æŠ¥å‘Š
    os.makedirs("reports", exist_ok=True)
    with open(f"reports/metrics_n{n_estimators}_d{max_depth}.json", "w") as f:
        json.dump(metrics, f, indent=2)

    print(f"âœ… è¯„ä¼°å®Œæˆ | MSE: {mse:.2f} | MAE: {mae:.2f} | RMSE: {rmse:.2f} | RÂ²: {r2:.4f}")
    return metrics


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--n_estimators", type=int, default=100)
    parser.add_argument("--max_depth", type=int, default=5)
    args = parser.parse_args()

    evaluate_model(args.n_estimators, args.max_depth)