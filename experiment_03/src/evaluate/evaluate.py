"""
æ¨¡å‹è¯„ä¼°ï¼šè®¡ç®— MSEã€RÂ² ç­‰æŒ‡æ ‡
"""
import joblib
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error
import json
import os


def evaluate_model():
    print("ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹...")
    model = joblib.load('models/rf_model.pkl')

    x_test = pd.read_csv('data/processed/x_test.csv')
    y_test = pd.read_csv('data/processed/y_test.csv').values.ravel()

    y_pred = model.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    metrics = {"mse": mse, "mae": mae, "rmse": rmse, "r2": r2}
    os.makedirs("reports", exist_ok=True)
    with open("reports/metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)

    print(f"âœ… è¯„ä¼°å®Œæˆ | MSE: {mse:.2f} | MAE: {mae:.2f} | RMSE: {rmse:.2f} | RÂ²: {r2:.4f}")
    return metrics


if __name__ == "__main__":
    evaluate_model()